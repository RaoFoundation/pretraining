{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SN 9 Demo\n",
    "\n",
    "## Overview\n",
    "\n",
    "The code and documentation for the subnet can be found at https://github.com/RaoFoundation/pretraining.\n",
    "\n",
    "Bittensor subnet 9 rewards miners for producing pretrained Foundation-Models on the Falcon Refined Web dataset. It acts like a continuous benchmark whereby miners are rewarded for attaining the best losses on randomly sampled pages of Falcon given a consistent model architecture.\n",
    "\n",
    "1. Miners train and periodically publish models to hugging face and commit the metadata for that model to the Bittensor chain. See https://github.com/RaoFoundation/pretraining/blob/main/docs/miner.md for more details.\n",
    "\n",
    "2. Validators download the models from hugging face for each miner based on the Bittensor chain metadata and continuously evaluate them, setting weights based on the performance of each model against the Falcon dataset. See https://github.com/RaoFoundation/pretraining/blob/main/docs/validator.md for more details.\n",
    "\n",
    "3. The Bittensor chain aggregates weights from all active validators using Yuma Consensus to determine the proportion of TAO emission rewarded to miners and validators. See https://docs.bittensor.com/learn/anatomy-of-incentive-mechanism for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best uploaded model\n",
    "\n",
    "The leaderboard is regularly updated with the best model as well as benchmark information for that model vs other well known models. https://huggingface.co/spaces/RaoFoundation/pretraining-leaderboard.\n",
    "\n",
    "From the leaderboard you can follow the links to the hugging face repo.\n",
    "\n",
    "Alternatively you can use the built in helpers in the following code block to programatically get the best_uid and the corresponding repository from the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretrain as pt\n",
    "\n",
    "best_uid = pt.graph.best_uid()\n",
    "repo_url = pt.mining.get_repo(best_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_directory = \"your/download/directory\"\n",
    "best_model = pt.mining.load_remote_model(best_uid, download_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reload the model with bfloat16 and flash_attention_2 for memory/performance efficiency:\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "best_model = AutoModelForCausalLM.from_pretrained(\n",
    "                pretrained_model_name_or_path=download_directory,\n",
    "                local_files_only=True,\n",
    "                use_safetensors=True,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference with the model\n",
    "\n",
    "Note that all models are using the same tokenizer. So we will go ahead and create that tokenizer to parse the desired prompts.\n",
    "\n",
    "You will need to run this notebook on a device with an nvidia GPU supporting bfloat16 and flash attention 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pt.model.get_tokenizer()\n",
    "\n",
    "prompt = \"\"\"Permaculture is a design process mimicking the diversity, functionality and resilience of natural ecosystems. The principles and practices are drawn from traditional ecological knowledge of indigenous cultures combined with modern scientific understanding and technological innovations. Permaculture design provides a framework helping individuals and communities develop innovative, creative and effective strategies for meeting basic needs while preparing for and mitigating the projected impacts of climate change.\n",
    "Write a summary of the above text.\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# Encode the prompt.\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the output token ids.\n",
    "ids = best_model.generate(inputs.input_ids.cuda(), num_return_sequences=1)\n",
    "\n",
    "# Decode the output tokens into text.\n",
    "text = tokenizer.decode(ids[0], skip_special_tokens=True)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Operations\n",
    "\n",
    "If you are interested in checking the specific metadata that has been uploaded to the chain that is something you can also do directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bittensor as bt\n",
    "from model.storage.chain.chain_model_metadata_store import ChainModelMetadataStore\n",
    "\n",
    "metadata_store = ChainModelMetadataStore(subtensor=bt.subtensor())\n",
    "\n",
    "model_metadata = await metadata_store.retrieve_model_metadata(\n",
    "    \"HOTKEY_TO_CHECK\"\n",
    ")\n",
    "print(model_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Loop\n",
    "\n",
    "1. Identifies valid models for evaluation (top N from last run + newly updated models).\n",
    "2. Generates random pages for evaluation and prepares batches for each page from the dataset.\n",
    "3. Computes the scoring for each model based on the losses incurred on the evaluation batches.\n",
    "4. Calculates wins and win rates for each model to determine their performance relative to others.\n",
    "5. Updates the weights of each model based on their performance and applies a softmax normalization.\n",
    "6. Implements a blacklist mechanism to remove underperforming models from the evaluation set.\n",
    "7. Logs all relevant data for the step, including model IDs, pages, batches, wins, win rates, and losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random pages for evaluation and prepares batches for each page from the dataset.\n",
    "import constants\n",
    "import random\n",
    "import pretrain as pt\n",
    "\n",
    "pages = [\n",
    "    random.randint(1, pt.dataset.SubsetFalconLoader.max_pages)\n",
    "    for _ in range(constants.n_eval_pages)\n",
    "]\n",
    "\n",
    "tokenizer = pt.model.get_tokenizer()\n",
    "loader = pt.dataset.SubsetFalconLoader(\n",
    "        batch_size=constants.batch_size,\n",
    "        sequence_length=constants.SEQUENCE_LENGTH_2,\n",
    "        pages=pages,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "batches = list(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the scoring for each model based on the losses incurred on the evaluation batches.\n",
    "\n",
    "# The validator will load each model in the current evaluation one by one.\n",
    "# This notebook uses the previously loaded best_model.\n",
    "\n",
    "losses = pt.validation.compute_losses(best_model, batches, \"cuda\", tokenizer.eos_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
